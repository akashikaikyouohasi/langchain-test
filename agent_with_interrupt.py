"""
LangGraphã‚’ä½¿ç”¨ã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…ï¼ˆinterruptç‰ˆï¼‰
- è¤‡æ•°ã®ãƒ„ãƒ¼ãƒ«
- Human in the Loopï¼ˆinterruptä½¿ç”¨ï¼‰
- æ§‹é€ åŒ–ã•ã‚ŒãŸå‡ºåŠ›
"""

import operator
import os
from typing import Annotated, Sequence, TypedDict, Literal
from pydantic import BaseModel, Field

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool
from langchain_aws import ChatBedrock

from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import interrupt


# ========== ãƒ„ãƒ¼ãƒ«ã®å®šç¾© ==========

@tool
def search_web(query: str) -> str:
    """Webã§æƒ…å ±ã‚’æ¤œç´¢ã—ã¾ã™ã€‚"""
    return f"'{query}'ã«ã¤ã„ã¦ã®æ¤œç´¢çµæœ: ã“ã‚Œã¯ã‚µãƒ³ãƒ—ãƒ«ã®æ¤œç´¢çµæœã§ã™ã€‚"


@tool
def calculator(expression: str) -> str:
    """æ•°å¼ã‚’è¨ˆç®—ã—ã¾ã™ã€‚ä¾‹: '2 + 2' ã‚„ '10 * 5'"""
    try:
        result = eval(expression, {"__builtins__": {}}, {})
        return f"è¨ˆç®—çµæœ: {result}"
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼: {str(e)}"


@tool
def get_current_info(topic: str) -> str:
    """ç‰¹å®šã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã®ç¾åœ¨ã®æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚"""
    return f"'{topic}'ã«ã¤ã„ã¦ã®ç¾åœ¨ã®æƒ…å ±: ã“ã‚Œã¯ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ã§ã™ã€‚"


tools = [search_web, calculator, get_current_info]


# ========== æ§‹é€ åŒ–ã•ã‚ŒãŸå‡ºåŠ›ã®å®šç¾© ==========

class FinalAnswer(BaseModel):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æœ€çµ‚çš„ãªæ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”"""
    summary: str = Field(description="ã‚¿ã‚¹ã‚¯ã®è¦ç´„")
    findings: list[str] = Field(description="ç™ºè¦‹ã—ãŸé‡è¦ãªæƒ…å ±ã®ãƒªã‚¹ãƒˆ")
    calculations: dict[str, float] = Field(
        default_factory=dict,
        description="å®Ÿè¡Œã—ãŸè¨ˆç®—ã¨ãã®çµæœ"
    )
    confidence: float = Field(
        ge=0.0, le=1.0,
        description="å›ç­”ã®ä¿¡é ¼åº¦ï¼ˆ0.0-1.0ï¼‰"
    )
    sources: list[str] = Field(
        default_factory=list,
        description="ä½¿ç”¨ã—ãŸæƒ…å ±æº"
    )


# ========== ã‚°ãƒ©ãƒ•ã®çŠ¶æ…‹å®šç¾© ==========

class AgentState(TypedDict):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®çŠ¶æ…‹"""
    messages: Annotated[Sequence[BaseMessage], operator.add]
    final_answer: FinalAnswer | None


# ========== ãƒãƒ¼ãƒ‰ã®å®šç¾© ==========

def agent_node(state: AgentState) -> dict:
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒãƒ¼ãƒ‰: LLMã‚’å‘¼ã³å‡ºã—ã¦ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ±ºå®š"""
    llm = ChatBedrock(
        model_id=os.getenv("AWS_BEDROCK_MODEL", "anthropic.claude-3-5-sonnet-20241022-v2:0"),
        model_kwargs={"temperature": 0}
    )
    llm_with_tools = llm.bind_tools(tools)
    
    messages = state["messages"]
    response = llm_with_tools.invoke(messages)
    return {"messages": [response]}


def should_continue(state: AgentState) -> Literal["tools", "human_review", "finalize"]:
    """æ¬¡ã«é€²ã‚€ã¹ããƒãƒ¼ãƒ‰ã‚’æ±ºå®š"""
    messages = state["messages"]
    last_message = messages[-1]
    
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        tool_names = [tc["name"] for tc in last_message.tool_calls]
        if "calculator" in tool_names:
            return "human_review"
        return "tools"
    
    return "finalize"


def human_review_node(state: AgentState) -> dict:
    """
    äººé–“ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å¾…ã¤ãƒãƒ¼ãƒ‰ï¼ˆinterruptä½¿ç”¨ï¼‰
    
    ã“ã®ãƒãƒ¼ãƒ‰ã¯interruptã‚’å‘¼ã³å‡ºã—ã€ã‚°ãƒ©ãƒ•ã‚’ä¸€æ™‚åœæ­¢ã—ã¾ã™ã€‚
    å¤–éƒ¨ã‹ã‚‰æ‰¿èª/æ‹’å¦ã®å¿œç­”ã‚’å—ã‘å–ã‚‹ã¾ã§å¾…æ©Ÿã—ã¾ã™ã€‚
    """
    messages = state["messages"]
    last_message = messages[-1]
    
    # ãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’å–å¾—
    tool_calls_info = []
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        for tool_call in last_message.tool_calls:
            tool_calls_info.append({
                "name": tool_call["name"],
                "args": tool_call["args"],
                "id": tool_call["id"]
            })
    
    # interrupt()ã‚’å‘¼ã³å‡ºã—ã¦ã‚°ãƒ©ãƒ•ã‚’ä¸€æ™‚åœæ­¢
    # æˆ»ã‚Šå€¤ã¨ã—ã¦æ‰¿èªãƒ‡ãƒ¼ã‚¿ã‚’æœŸå¾…
    approval_data = interrupt({
        "type": "human_review",
        "tool_calls": tool_calls_info,
        "message": "ãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡Œã«ã¯æ‰¿èªãŒå¿…è¦ã§ã™"
    })
    
    # approval_dataã®å½¢å¼:
    # {"approved": True} ã¾ãŸã¯ {"approved": False, "feedback": "..."}
    
    if approval_data.get("approved"):
        # æ‰¿èªã•ã‚ŒãŸå ´åˆã€ä½•ã‚‚è¿”ã•ãªã„ï¼ˆtoolsãƒãƒ¼ãƒ‰ã¸é€²ã‚€ï¼‰
        return {}
    else:
        # æ‹’å¦ã•ã‚ŒãŸå ´åˆã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¿½åŠ 
        feedback = approval_data.get("feedback", "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ‹’å¦ã—ã¾ã—ãŸ")
        return {
            "messages": [
                ToolMessage(
                    content=f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯: {feedback}",
                    tool_call_id=tool_calls_info[0]["id"]
                )
            ]
        }


def finalize_node(state: AgentState) -> dict:
    """æœ€çµ‚çš„ãªæ§‹é€ åŒ–ã•ã‚ŒãŸå‡ºåŠ›ã‚’ç”Ÿæˆ"""
    llm = ChatBedrock(
        model_id=os.getenv("AWS_BEDROCK_MODEL", "anthropic.claude-3-5-sonnet-20241022-v2:0"),
        model_kwargs={"temperature": 0}
    )
    
    structured_llm = llm.with_structured_output(FinalAnswer)
    
    messages = state["messages"]
    final_prompt = HumanMessage(
        content="ã“ã‚Œã¾ã§ã®ä¼šè©±å†…å®¹ã‚’åŸºã«ã€æ§‹é€ åŒ–ã•ã‚ŒãŸæœ€çµ‚å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
    )
    
    final_answer = structured_llm.invoke(list(messages) + [final_prompt])
    
    return {
        "final_answer": final_answer,
        "messages": [AIMessage(content="æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")]
    }


# ========== ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰ ==========

def create_agent_graph():
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚°ãƒ©ãƒ•ã‚’ä½œæˆ"""
    tool_node = ToolNode(tools)
    workflow = StateGraph(AgentState)
    
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", tool_node)
    workflow.add_node("human_review", human_review_node)
    workflow.add_node("finalize", finalize_node)
    
    workflow.set_entry_point("agent")
    
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            "human_review": "human_review",
            "finalize": "finalize"
        }
    )
    
    workflow.add_edge("human_review", "tools")
    workflow.add_edge("tools", "agent")
    workflow.add_edge("finalize", END)
    
    # ãƒ¡ãƒ¢ãƒªã‚’è¿½åŠ ï¼ˆçŠ¶æ…‹ã‚’æ°¸ç¶šåŒ–ï¼‰
    memory = MemorySaver()
    
    return workflow.compile(checkpointer=memory)


# ========== ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œï¼ˆã‚³ãƒ³ã‚½ãƒ¼ãƒ«ç‰ˆï¼‰ ==========

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°ï¼ˆã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰"""
    print("LangGraph ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ with Interrupts")
    print("="*50)
    
    app = create_agent_graph()
    
    initial_message = input("\nã‚¿ã‚¹ã‚¯ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ > ")
    
    config = {"configurable": {"thread_id": "1"}}
    
    initial_state = {
        "messages": [HumanMessage(content=initial_message)],
        "final_answer": None
    }
    
    print("\nå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...\n")
    
    # ã‚°ãƒ©ãƒ•ã‚’å®Ÿè¡Œ
    current_state = initial_state
    
    while True:
        try:
            # ã‚°ãƒ©ãƒ•ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ å®Ÿè¡Œ
            result = None
            for event in app.stream(current_state, config, stream_mode="values"):
                result = event
                
                # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
                if "messages" in event and event["messages"]:
                    last_msg = event["messages"][-1]
                    if hasattr(last_msg, "content") and last_msg.content:
                        print(f"ğŸ’¬ {type(last_msg).__name__}: {last_msg.content[:100]}...")
            
            # å®Œäº†ã—ãŸå ´åˆ
            if result and result.get("final_answer"):
                print("\n" + "="*50)
                print("âœ… æœ€çµ‚çš„ãªæ§‹é€ åŒ–ã•ã‚ŒãŸå‡ºåŠ›")
                print("="*50)
                final_answer = result["final_answer"]
                print(f"\nè¦ç´„: {final_answer.summary}")
                print(f"\nç™ºè¦‹äº‹é …:")
                for i, finding in enumerate(final_answer.findings, 1):
                    print(f"  {i}. {finding}")
                print(f"\nè¨ˆç®—çµæœ: {final_answer.calculations}")
                print(f"ä¿¡é ¼åº¦: {final_answer.confidence}")
                print(f"æƒ…å ±æº: {final_answer.sources}")
                break
                
        except Exception as e:
            # interruptã«ã‚ˆã‚‹ä¸­æ–­ã‚’ã‚­ãƒ£ãƒƒãƒ
            if "interrupt" in str(type(e)).lower() or hasattr(e, '__cause__'):
                # æœ€æ–°ã®çŠ¶æ…‹ã‚’å–å¾—
                snapshot = app.get_state(config)
                
                # interruptæƒ…å ±ã‚’å–å¾—
                if snapshot.tasks:
                    task = snapshot.tasks[0]
                    interrupt_data = task.interrupts[0].value if task.interrupts else None
                    
                    if interrupt_data:
                        print("\n" + "="*50)
                        print("ğŸ” Human Review Required")
                        print("="*50)
                        
                        for tool_call in interrupt_data.get("tool_calls", []):
                            print(f"\nãƒ„ãƒ¼ãƒ«: {tool_call['name']}")
                            print(f"å¼•æ•°: {tool_call['args']}")
                        
                        print("\næ‰¿èªã—ã¾ã™ã‹ï¼Ÿ")
                        print("  y/yes: æ‰¿èªã—ã¦ç¶šè¡Œ")
                        print("  n/no: æ‹’å¦ã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å…¥åŠ›")
                        
                        user_input = input("\nå…¥åŠ› > ").strip().lower()
                        
                        if user_input in ["y", "yes"]:
                            # æ‰¿èªã—ã¦å†é–‹
                            app.update_state(
                                config,
                                {"approved": True},
                                as_node="human_review"
                            )
                            current_state = None  # æ—¢å­˜ã®çŠ¶æ…‹ã‹ã‚‰ç¶šè¡Œ
                        else:
                            # æ‹’å¦ã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
                            feedback = input("ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ > ")
                            app.update_state(
                                config,
                                {"approved": False, "feedback": feedback},
                                as_node="human_review"
                            )
                            current_state = None
                    else:
                        break
                else:
                    break
            else:
                print(f"\nã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                import traceback
                traceback.print_exc()
                break


if __name__ == "__main__":
    from dotenv import load_dotenv
    
    load_dotenv()
    
    if not os.getenv("AWS_BEDROCK_MODEL"):
        print("âš ï¸  AWS Bedrockã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        print(".envãƒ•ã‚¡ã‚¤ãƒ«ã§AWS_BEDROCK_MODELã‚’è¨­å®šã™ã‚‹ã‹ã€")
        print("AWS CLIã§èªè¨¼æƒ…å ±ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    else:
        main()
