"""
LangGraphã‚’ä½¿ç”¨ã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè£…ï¼ˆinterruptç‰ˆï¼‰
- è¤‡æ•°ã®ãƒ„ãƒ¼ãƒ«
- Human in the Loopï¼ˆinterruptä½¿ç”¨ï¼‰
- æ§‹é€ åŒ–ã•ã‚ŒãŸå‡ºåŠ›
"""

import operator
import os
from typing import Annotated, Sequence, TypedDict, Literal
from pydantic import BaseModel, Field

from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.tools import tool
from langchain_aws import ChatBedrock

from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import interrupt


# ========== ãƒ„ãƒ¼ãƒ«ã®å®šç¾© ==========

@tool
def search_web(query: str) -> str:
    """Webã§æƒ…å ±ã‚’æ¤œç´¢ã—ã¾ã™ã€‚"""
    # å®Ÿéš›ã®Webæ¤œç´¢APIã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã“ã“ã‚’å®Ÿè£…
    return f"'{query}'ã«ã¤ã„ã¦ã®æ¤œç´¢çµæœ: ã“ã‚Œã¯ã‚µãƒ³ãƒ—ãƒ«ã®æ¤œç´¢çµæœã§ã™ã€‚"


@tool
def calculator(expression: str) -> str:
    """æ•°å¼ã‚’è¨ˆç®—ã—ã¾ã™ã€‚ä¾‹: '2 + 2' ã‚„ '10 * 5'"""
    try:
        # å®‰å…¨æ€§ã®ãŸã‚ã€evalã®ä»£ã‚ã‚Šã«åˆ¶é™ã•ã‚ŒãŸè¨ˆç®—ã‚’è¡Œã†
        result = eval(expression, {"__builtins__": {}}, {})
        return f"è¨ˆç®—çµæœ: {result}"
    except Exception as e:
        return f"ã‚¨ãƒ©ãƒ¼: {str(e)}"


@tool
def get_current_info(topic: str) -> str:
    """ç‰¹å®šã®ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã®ç¾åœ¨ã®æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚"""
    return f"'{topic}'ã«ã¤ã„ã¦ã®ç¾åœ¨ã®æƒ…å ±: ã“ã‚Œã¯ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ã§ã™ã€‚"


# ãƒ„ãƒ¼ãƒ«ã®ãƒªã‚¹ãƒˆ
tools = [search_web, calculator, get_current_info]


# ========== æ§‹é€ åŒ–ã•ã‚ŒãŸå‡ºåŠ›ã®å®šç¾© ==========

class FinalAnswer(BaseModel):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æœ€çµ‚çš„ãªæ§‹é€ åŒ–ã•ã‚ŒãŸå›ç­”"""
    summary: str = Field(description="ã‚¿ã‚¹ã‚¯ã®è¦ç´„")
    findings: list[str] = Field(description="ç™ºè¦‹ã—ãŸé‡è¦ãªæƒ…å ±ã®ãƒªã‚¹ãƒˆ")
    calculations: dict[str, float] = Field(
        default_factory=dict,
        description="å®Ÿè¡Œã—ãŸè¨ˆç®—ã¨ãã®çµæœ"
    )
    confidence: float = Field(
        ge=0.0, le=1.0,
        description="å›ç­”ã®ä¿¡é ¼åº¦ï¼ˆ0.0-1.0ï¼‰"
    )
    sources: list[str] = Field(
        default_factory=list,
        description="ä½¿ç”¨ã—ãŸæƒ…å ±æº"
    )


# ========== ã‚°ãƒ©ãƒ•ã®çŠ¶æ…‹å®šç¾© ==========

class AgentState(TypedDict):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®çŠ¶æ…‹"""
    messages: Annotated[Sequence[BaseMessage], operator.add]
    final_answer: FinalAnswer | None


# ========== ãƒãƒ¼ãƒ‰ã®å®šç¾© ==========

def agent_node(state: AgentState) -> dict:
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒãƒ¼ãƒ‰: LLMã‚’å‘¼ã³å‡ºã—ã¦ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ±ºå®š"""
    llm = ChatBedrock(
        model_id=os.getenv("AWS_BEDROCK_MODEL", "anthropic.claude-3-5-sonnet-20241022-v2:0"),
        model_kwargs={"temperature": 0}
    )
    llm_with_tools = llm.bind_tools(tools)
    
    messages = state["messages"]
    
    # å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ­ã‚°å‡ºåŠ›
    print("\n" + "="*50)
    print("ğŸ¤– Agent Node - å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    print("="*50)
    for i, msg in enumerate(messages, 1):
        msg_type = type(msg).__name__
        print(f"\n[ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {i}] {msg_type}")
        if hasattr(msg, "content") and msg.content:
            print(f"Content: {msg.content}")
        if hasattr(msg, "tool_calls") and msg.tool_calls:
            print(f"Tool Calls: {msg.tool_calls}")
        if isinstance(msg, ToolMessage):
            print(f"Tool Call ID: {msg.tool_call_id}")
    
    # LLMã‚’å‘¼ã³å‡ºã—
    response = llm_with_tools.invoke(messages)
    
    # å‡ºåŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ­ã‚°å‡ºåŠ›
    print("\n" + "="*50)
    print("ğŸ¤– Agent Node - å‡ºåŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    print("="*50)
    print(f"Response Type: {type(response).__name__}")
    if hasattr(response, "content") and response.content:
        print(f"Content: {response.content}")
    if hasattr(response, "tool_calls") and response.tool_calls:
        print(f"\nTool Calls ({len(response.tool_calls)}ä»¶):")
        for tc in response.tool_calls:
            print(f"  - ãƒ„ãƒ¼ãƒ«: {tc['name']}")
            print(f"    å¼•æ•°: {tc['args']}")
            print(f"    ID: {tc['id']}")
    print("="*50 + "\n")
    
    return {"messages": [response]}


def should_continue(state: AgentState) -> Literal["tools", "human_review", "finalize"]:
    """æ¬¡ã«é€²ã‚€ã¹ããƒãƒ¼ãƒ‰ã‚’æ±ºå®š"""
    messages = state["messages"]
    last_message = messages[-1]
    
    # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒã‚ã‚‹å ´åˆ
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        # é‡è¦ãªæ“ä½œï¼ˆä¾‹: calculatorï¼‰ã®å ´åˆã¯äººé–“ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¦æ±‚
        tool_names = [tc["name"] for tc in last_message.tool_calls]
        if "calculator" in tool_names:
            return "human_review"
        return "tools"
    
    # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒãªã„å ´åˆã¯æœ€çµ‚åŒ–
    return "finalize"


def human_review_node(state: AgentState) -> dict:
    """
    äººé–“ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å¾…ã¤ãƒãƒ¼ãƒ‰ï¼ˆinterruptä½¿ç”¨ï¼‰
    
    ã“ã®ãƒãƒ¼ãƒ‰ã¯interrupt()ã‚’å‘¼ã³å‡ºã—ã¦ã‚°ãƒ©ãƒ•ã‚’ä¸€æ™‚åœæ­¢ã—ã¾ã™ã€‚
    å¤–éƒ¨ã‹ã‚‰æ‰¿èª/æ‹’å¦ã®å¿œç­”ã‚’å—ã‘å–ã‚‹ã¾ã§å¾…æ©Ÿã—ã¾ã™ã€‚
    """
    messages = state["messages"]
    last_message = messages[-1]
    
    # ãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’å–å¾—
    tool_calls_info = []
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        for tool_call in last_message.tool_calls:
            tool_calls_info.append({
                "name": tool_call["name"],
                "args": tool_call["args"],
                "id": tool_call["id"]
            })
    
    # interrupt()ã‚’å‘¼ã³å‡ºã—ã¦ã‚°ãƒ©ãƒ•ã‚’ä¸€æ™‚åœæ­¢
    # æˆ»ã‚Šå€¤ã¨ã—ã¦æ‰¿èªãƒ‡ãƒ¼ã‚¿ã‚’æœŸå¾…
    approval_data = interrupt({
        "type": "human_review",
        "tool_calls": tool_calls_info,
        "message": "ãƒ„ãƒ¼ãƒ«ã®å®Ÿè¡Œã«ã¯æ‰¿èªãŒå¿…è¦ã§ã™"
    })
    
    # approval_dataã®å½¢å¼:
    # {"approved": True} ã¾ãŸã¯ {"approved": False, "feedback": "..."}
    
    if approval_data.get("approved"):
        # æ‰¿èªã•ã‚ŒãŸå ´åˆã€ä½•ã‚‚è¿”ã•ãªã„ï¼ˆæ¬¡ã®ãƒãƒ¼ãƒ‰ã¸é€²ã‚€ï¼‰
        return {}
    else:
        # æ‹’å¦ã•ã‚ŒãŸå ´åˆã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¿½åŠ 
        feedback = approval_data.get("feedback", "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ‹’å¦ã—ã¾ã—ãŸ")
        return {
            "messages": [
                ToolMessage(
                    content=f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯: {feedback}",
                    tool_call_id=tool_calls_info[0]["id"]
                )
            ]
        }


def finalize_node(state: AgentState) -> dict:
    """æœ€çµ‚çš„ãªæ§‹é€ åŒ–ã•ã‚ŒãŸå‡ºåŠ›ã‚’ç”Ÿæˆ"""
    llm = ChatBedrock(
        model_id=os.getenv("AWS_BEDROCK_MODEL", "anthropic.claude-3-5-sonnet-20241022-v2:0"),
        model_kwargs={"temperature": 0}
    )
    
    # Pydanticãƒ¢ãƒ‡ãƒ«ã§æ§‹é€ åŒ–ã•ã‚ŒãŸå‡ºåŠ›ã‚’ç”Ÿæˆ
    structured_llm = llm.with_structured_output(FinalAnswer)
    
    # ä¼šè©±å±¥æ­´ã‹ã‚‰æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆ
    messages = state["messages"]
    final_prompt = HumanMessage(
        content="ã“ã‚Œã¾ã§ã®ä¼šè©±å†…å®¹ã‚’åŸºã«ã€æ§‹é€ åŒ–ã•ã‚ŒãŸæœ€çµ‚å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
    )
    
    # å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ­ã‚°å‡ºåŠ›
    print("\n" + "="*50)
    print("ğŸ“ Finalize Node - å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    print("="*50)
    print(f"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´: {len(messages)}ä»¶")
    for i, msg in enumerate(messages, 1):
        msg_type = type(msg).__name__
        print(f"\n[ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {i}] {msg_type}")
        if hasattr(msg, "content") and msg.content:
            content_preview = msg.content[:100] + "..." if len(msg.content) > 100 else msg.content
            print(f"Content: {content_preview}")
        if hasattr(msg, "tool_calls") and msg.tool_calls:
            print(f"Tool Calls: {len(msg.tool_calls)}ä»¶")
        if isinstance(msg, ToolMessage):
            print(f"Tool Call ID: {msg.tool_call_id}")
    
    print(f"\n[è¿½åŠ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ] {type(final_prompt).__name__}")
    print(f"Content: {final_prompt.content}")
    print("="*50)
    
    # LLMã‚’å‘¼ã³å‡ºã—
    final_answer = structured_llm.invoke(list(messages) + [final_prompt])
    
    # å‡ºåŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæ§‹é€ åŒ–ã•ã‚ŒãŸçµæœï¼‰ã‚’ãƒ­ã‚°å‡ºåŠ›
    print("\n" + "="*50)
    print("ğŸ“ Finalize Node - å‡ºåŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæ§‹é€ åŒ–ã•ã‚ŒãŸçµæœï¼‰")
    print("="*50)
    print(f"å‹: {type(final_answer).__name__}")
    print(f"\nè¦ç´„: {final_answer.summary}")
    print(f"\nç™ºè¦‹äº‹é … ({len(final_answer.findings)}ä»¶):")
    for i, finding in enumerate(final_answer.findings, 1):
        print(f"  {i}. {finding}")
    print(f"\nè¨ˆç®—çµæœ: {final_answer.calculations}")
    print(f"ä¿¡é ¼åº¦: {final_answer.confidence}")
    print(f"æƒ…å ±æº: {final_answer.sources}")
    print("="*50 + "\n")
    
    return {
        "final_answer": final_answer,
        "messages": [AIMessage(content="æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")]
    }


# ========== ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰ ==========

def create_agent_graph():
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚°ãƒ©ãƒ•ã‚’ä½œæˆ"""
    # ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ãƒ‰ã‚’ä½œæˆ
    tool_node = ToolNode(tools)
    
    # ã‚°ãƒ©ãƒ•ã‚’åˆæœŸåŒ–
    workflow = StateGraph(AgentState)
    
    # ãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", tool_node)
    workflow.add_node("human_review", human_review_node)
    workflow.add_node("finalize", finalize_node)
    
    # ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’è¨­å®š
    workflow.set_entry_point("agent")
    
    # ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            "human_review": "human_review",
            "finalize": "finalize"
        }
    )
    
    # human_reviewã®å¾Œã¯toolsã¸ï¼ˆæ‰¿èªæ™‚ï¼‰ã¾ãŸã¯agentã¸ï¼ˆæ‹’å¦æ™‚ï¼‰
    # human_review_nodeã®è¿”ã‚Šå€¤ã§åˆ¤æ–­
    def after_human_review(state: AgentState) -> Literal["tools", "agent"]:
        messages = state["messages"]
        if messages:
            last_msg = messages[-1]
            # ToolMessageãŒã‚ã‚‹ = æ‹’å¦ã•ã‚ŒãŸ
            if isinstance(last_msg, ToolMessage):
                return "agent"
        # ãã‚Œä»¥å¤–ã¯æ‰¿èª = toolsã¸
        return "tools"
    
    workflow.add_conditional_edges(
        "human_review",
        after_human_review,
        {
            "tools": "tools",
            "agent": "agent"
        }
    )
    
    # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå¾Œã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«æˆ»ã‚‹
    workflow.add_edge("tools", "agent")
    
    # æœ€çµ‚åŒ–å¾Œã¯çµ‚äº†
    workflow.add_edge("finalize", END)
    
    # ãƒ¡ãƒ¢ãƒªã‚’è¿½åŠ ï¼ˆä¼šè©±ã®çŠ¶æ…‹ã‚’ä¿æŒï¼‰
    memory = MemorySaver()
    
    return workflow.compile(checkpointer=memory)


# ========== ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ ==========

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°ï¼ˆinterruptå¯¾å¿œï¼‰"""
    print("LangGraph ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ with Human-in-the-Loop (Interruptç‰ˆ)")
    print("="*50)
    
    # ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ
    app = create_agent_graph()
    
    # åˆæœŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    initial_message = input("\nã‚¿ã‚¹ã‚¯ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ > ")
    
    # è¨­å®š
    config = {"configurable": {"thread_id": "1"}}
    
    # åˆæœŸçŠ¶æ…‹
    initial_state = {
        "messages": [HumanMessage(content=initial_message)],
        "final_answer": None
    }
    
    print("\nå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...\n")
    
    # ã‚°ãƒ©ãƒ•ã‚’å®Ÿè¡Œï¼ˆinterruptå¯¾å¿œãƒ«ãƒ¼ãƒ—ï¼‰
    current_state = initial_state
    
    while True:
        # ã‚°ãƒ©ãƒ•ã‚’å®Ÿè¡Œ
        for event in app.stream(current_state, config, stream_mode="values"):
            # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
            if "messages" in event and event["messages"]:
                last_msg = event["messages"][-1]
                msg_type = type(last_msg).__name__
                if hasattr(last_msg, "content") and last_msg.content:
                    print(f"ğŸ’¬ {msg_type}: {last_msg.content[:100]}...")
                elif hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
                    print(f"ğŸ”§ {msg_type}: ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã— {len(last_msg.tool_calls)}ä»¶")
            
            # æœ€çµ‚çµæœã‚’ãƒã‚§ãƒƒã‚¯
            if event.get("final_answer"):
                print("\n" + "="*50)
                print("âœ… æœ€çµ‚çš„ãªæ§‹é€ åŒ–ã•ã‚ŒãŸå‡ºåŠ›")
                print("="*50)
                final_answer = event["final_answer"]
                print(f"\nè¦ç´„: {final_answer.summary}")
                print(f"\nç™ºè¦‹äº‹é …:")
                for i, finding in enumerate(final_answer.findings, 1):
                    print(f"  {i}. {finding}")
                print(f"\nè¨ˆç®—çµæœ: {final_answer.calculations}")
                print(f"ä¿¡é ¼åº¦: {final_answer.confidence}")
                print(f"æƒ…å ±æº: {final_answer.sources}")
                return
        
        # interruptãŒç™ºç”Ÿã—ãŸã‹ãƒã‚§ãƒƒã‚¯
        snapshot = app.get_state(config)
        
        if not snapshot.next:
            # æ¬¡ã®ãƒãƒ¼ãƒ‰ãŒãªã„ = å®Œäº†
            break
        
        # interruptãŒç™ºç”Ÿã—ã¦ã„ã‚‹å ´åˆ
        if snapshot.tasks:
            task = snapshot.tasks[0]
            if task.interrupts:
                interrupt_value = task.interrupts[0].value
                
                print("\n" + "="*50)
                print("ğŸ” Human Review Required")
                print("="*50)
                
                # ãƒ„ãƒ¼ãƒ«æƒ…å ±ã‚’è¡¨ç¤º
                for tool_call in interrupt_value.get("tool_calls", []):
                    print(f"\nãƒ„ãƒ¼ãƒ«: {tool_call['name']}")
                    print(f"å¼•æ•°: {tool_call['args']}")
                
                print("\næ‰¿èªã—ã¾ã™ã‹ï¼Ÿ")
                print("  y/yes: æ‰¿èªã—ã¦ç¶šè¡Œ")
                print("  n/no: æ‹’å¦ã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å…¥åŠ›")
                
                user_input = input("\nå…¥åŠ› > ").strip().lower()
                
                if user_input in ["y", "yes"]:
                    # æ‰¿èªã—ã¦å†é–‹
                    app.update_state(
                        config,
                        {"approved": True},
                        as_node="human_review"
                    )
                else:
                    # æ‹’å¦ã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
                    feedback = input("ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ > ")
                    app.update_state(
                        config,
                        {"approved": False, "feedback": feedback},
                        as_node="human_review"
                    )
                
                # æ¬¡ã®ãƒ«ãƒ¼ãƒ—ã§ç¶šè¡Œ
                current_state = None
                continue
        
        # interruptãŒãªã„å ´åˆã¯çµ‚äº†
        break


if __name__ == "__main__":
    from dotenv import load_dotenv
    
    # ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
    load_dotenv()
    
    if not os.getenv("AWS_BEDROCK_MODEL"):
        print("âš ï¸  AWS Bedrockã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        print(".envãƒ•ã‚¡ã‚¤ãƒ«ã§AWS_BEDROCK_MODELã‚’è¨­å®šã™ã‚‹ã‹ã€")
        print("AWS CLIã§èªè¨¼æƒ…å ±ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
        print("\nAWS CLIã®è¨­å®š: aws configure")
        print("\nğŸ’¡ interruptç‰ˆã®å®Ÿè£…:")
        print("   - ã‚°ãƒ©ãƒ•ãŒä¸€æ™‚åœæ­¢ã—ã€å¤–éƒ¨ã‹ã‚‰å†é–‹å¯èƒ½")
        print("   - Web UIã€APIã€Slackãªã©ã¨çµ±åˆå¯èƒ½")
        print("   - çŠ¶æ…‹ãŒæ°¸ç¶šåŒ–ã•ã‚Œã€å¾Œã‹ã‚‰å†é–‹å¯èƒ½")
    else:
        main()
